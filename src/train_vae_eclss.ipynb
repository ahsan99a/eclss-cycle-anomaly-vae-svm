{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50183066-3347-46c1-a400-62f6819552f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      " DENSE VAE TRAINING FOR ECLSS ANOMALY DETECTION\n",
      "============================================================\n",
      "Using device: cpu\n",
      "Preprocessed data dir: C:\\Users\\ahasa\\project_root\\data\\eclss_preprocessed\n",
      "\n",
      "Shapes:\n",
      "  X_train_nom: (84, 3000)\n",
      "  X_val_nom:   (18, 3000)\n",
      "  X_test_all:  (198, 3000)\n",
      "  y_test_all:  (198,)\n",
      "\n",
      "DenseVAE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=3000, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): LeakyReLU(negative_slope=0.2)\n",
      "    (7): Dropout(p=0.1, inplace=False)\n",
      "    (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2)\n",
      "    (11): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=256, out_features=32, bias=True)\n",
      "  (fc_logvar): Linear(in_features=256, out_features=32, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2)\n",
      "    (6): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (7): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.2)\n",
      "    (9): Linear(in_features=1024, out_features=3000, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Number of trainable parameters: 7493112\n",
      "\n",
      "Epoch 001 | Train: loss=1.38868, recon=0.53511, KL=2.84524 | Val: loss=0.81166, recon=0.57523, KL=0.78807\n",
      "Epoch 002 | Train: loss=0.73317, recon=0.30261, KL=1.43520 | Val: loss=0.71156, recon=0.46809, KL=0.81157\n",
      "Epoch 003 | Train: loss=0.50968, recon=0.27818, KL=0.77166 | Val: loss=0.47976, recon=0.38683, KL=0.30974\n",
      "Epoch 004 | Train: loss=0.43835, recon=0.27070, KL=0.55885 | Val: loss=0.38311, recon=0.33787, KL=0.15080\n",
      "Epoch 005 | Train: loss=0.40747, recon=0.25684, KL=0.50211 | Val: loss=0.34663, recon=0.31880, KL=0.09278\n",
      "Epoch 006 | Train: loss=0.37759, recon=0.24007, KL=0.45840 | Val: loss=0.33571, recon=0.31400, KL=0.07237\n",
      "Epoch 007 | Train: loss=0.36967, recon=0.23702, KL=0.44214 | Val: loss=0.32515, recon=0.30172, KL=0.07809\n",
      "Epoch 008 | Train: loss=0.35943, recon=0.23250, KL=0.42307 | Val: loss=0.34382, recon=0.32347, KL=0.06784\n",
      "Epoch 009 | Train: loss=0.35659, recon=0.23721, KL=0.39793 | Val: loss=0.33051, recon=0.30654, KL=0.07990\n",
      "Epoch 010 | Train: loss=0.36232, recon=0.24426, KL=0.39353 | Val: loss=0.34415, recon=0.33049, KL=0.04555\n",
      "Epoch 011 | Train: loss=0.34509, recon=0.23397, KL=0.37041 | Val: loss=0.32273, recon=0.30698, KL=0.05251\n",
      "Epoch 012 | Train: loss=0.34142, recon=0.23515, KL=0.35424 | Val: loss=0.32152, recon=0.30593, KL=0.05200\n",
      "Epoch 013 | Train: loss=0.34035, recon=0.23755, KL=0.34266 | Val: loss=0.31145, recon=0.29684, KL=0.04870\n",
      "Epoch 014 | Train: loss=0.31749, recon=0.22632, KL=0.30390 | Val: loss=0.31068, recon=0.29906, KL=0.03873\n",
      "Epoch 015 | Train: loss=0.30833, recon=0.21560, KL=0.30909 | Val: loss=0.30880, recon=0.29725, KL=0.03851\n",
      "Epoch 016 | Train: loss=0.30715, recon=0.22031, KL=0.28947 | Val: loss=0.31207, recon=0.29613, KL=0.05315\n",
      "Epoch 017 | Train: loss=0.32786, recon=0.23297, KL=0.31630 | Val: loss=0.32518, recon=0.31436, KL=0.03605\n",
      "Epoch 018 | Train: loss=0.30753, recon=0.22687, KL=0.26888 | Val: loss=0.31166, recon=0.29918, KL=0.04160\n",
      "Epoch 019 | Train: loss=0.30665, recon=0.22243, KL=0.28073 | Val: loss=0.30589, recon=0.29248, KL=0.04469\n",
      "Epoch 020 | Train: loss=0.34553, recon=0.26372, KL=0.27270 | Val: loss=0.31566, recon=0.30555, KL=0.03372\n",
      "Epoch 021 | Train: loss=0.29624, recon=0.22229, KL=0.24649 | Val: loss=0.29640, recon=0.28388, KL=0.04175\n",
      "Epoch 022 | Train: loss=0.28919, recon=0.21236, KL=0.25612 | Val: loss=0.28778, recon=0.27808, KL=0.03236\n",
      "Epoch 023 | Train: loss=0.29627, recon=0.22427, KL=0.24000 | Val: loss=0.28958, recon=0.27934, KL=0.03414\n",
      "Epoch 024 | Train: loss=0.27902, recon=0.21156, KL=0.22487 | Val: loss=0.29103, recon=0.28068, KL=0.03450\n",
      "Epoch 025 | Train: loss=0.28309, recon=0.21253, KL=0.23520 | Val: loss=0.29414, recon=0.28409, KL=0.03349\n",
      "Epoch 026 | Train: loss=0.28166, recon=0.21249, KL=0.23057 | Val: loss=0.28030, recon=0.27027, KL=0.03345\n",
      "Epoch 027 | Train: loss=0.27317, recon=0.21269, KL=0.20163 | Val: loss=0.29050, recon=0.28294, KL=0.02520\n",
      "Epoch 028 | Train: loss=0.27161, recon=0.21724, KL=0.18120 | Val: loss=0.28740, recon=0.27928, KL=0.02707\n",
      "Epoch 029 | Train: loss=0.26363, recon=0.20581, KL=0.19274 | Val: loss=0.28837, recon=0.28071, KL=0.02553\n",
      "Epoch 030 | Train: loss=0.29652, recon=0.24426, KL=0.17420 | Val: loss=0.28552, recon=0.27444, KL=0.03693\n",
      "Epoch 031 | Train: loss=0.26180, recon=0.21071, KL=0.17029 | Val: loss=0.28035, recon=0.27323, KL=0.02371\n",
      "Epoch 032 | Train: loss=0.25509, recon=0.20465, KL=0.16813 | Val: loss=0.30390, recon=0.29802, KL=0.01959\n",
      "Epoch 033 | Train: loss=0.25689, recon=0.21174, KL=0.15052 | Val: loss=0.28821, recon=0.28289, KL=0.01772\n",
      "Epoch 034 | Train: loss=0.24781, recon=0.20266, KL=0.15050 | Val: loss=0.28191, recon=0.27678, KL=0.01709\n",
      "Epoch 035 | Train: loss=0.25264, recon=0.20804, KL=0.14867 | Val: loss=0.28126, recon=0.27578, KL=0.01828\n",
      "Epoch 036 | Train: loss=0.24516, recon=0.20488, KL=0.13427 | Val: loss=0.29008, recon=0.28494, KL=0.01715\n",
      "Epoch 037 | Train: loss=0.24664, recon=0.20367, KL=0.14324 | Val: loss=0.28866, recon=0.28361, KL=0.01683\n",
      "Epoch 038 | Train: loss=0.25066, recon=0.21297, KL=0.12564 | Val: loss=0.29211, recon=0.28791, KL=0.01399\n",
      "Epoch 039 | Train: loss=0.24892, recon=0.21286, KL=0.12020 | Val: loss=0.27585, recon=0.27212, KL=0.01243\n",
      "Epoch 040 | Train: loss=0.24251, recon=0.20431, KL=0.12733 | Val: loss=0.29901, recon=0.29494, KL=0.01357\n",
      "Epoch 041 | Train: loss=0.24170, recon=0.20718, KL=0.11506 | Val: loss=0.28596, recon=0.28219, KL=0.01255\n",
      "Epoch 042 | Train: loss=0.23082, recon=0.19915, KL=0.10557 | Val: loss=0.28065, recon=0.27661, KL=0.01344\n",
      "Epoch 043 | Train: loss=0.23303, recon=0.20070, KL=0.10777 | Val: loss=0.27370, recon=0.26989, KL=0.01269\n",
      "Epoch 044 | Train: loss=0.23253, recon=0.20325, KL=0.09759 | Val: loss=0.26941, recon=0.26634, KL=0.01026\n",
      "Epoch 045 | Train: loss=0.23323, recon=0.20116, KL=0.10689 | Val: loss=0.27810, recon=0.27485, KL=0.01084\n",
      "Epoch 046 | Train: loss=0.22845, recon=0.19794, KL=0.10171 | Val: loss=0.28245, recon=0.27903, KL=0.01140\n",
      "Epoch 047 | Train: loss=0.23480, recon=0.20578, KL=0.09672 | Val: loss=0.28033, recon=0.27740, KL=0.00974\n",
      "Epoch 048 | Train: loss=0.23048, recon=0.20233, KL=0.09382 | Val: loss=0.27399, recon=0.27068, KL=0.01105\n",
      "Epoch 049 | Train: loss=0.23825, recon=0.21302, KL=0.08409 | Val: loss=0.26721, recon=0.26436, KL=0.00951\n",
      "Epoch 050 | Train: loss=0.22671, recon=0.20438, KL=0.07442 | Val: loss=0.28768, recon=0.28548, KL=0.00733\n",
      "Epoch 051 | Train: loss=0.22931, recon=0.20480, KL=0.08172 | Val: loss=0.27255, recon=0.27006, KL=0.00829\n",
      "Epoch 052 | Train: loss=0.25777, recon=0.23486, KL=0.07637 | Val: loss=0.27229, recon=0.26971, KL=0.00862\n",
      "Epoch 053 | Train: loss=0.24217, recon=0.21874, KL=0.07813 | Val: loss=0.27221, recon=0.26977, KL=0.00814\n",
      "Epoch 054 | Train: loss=0.22283, recon=0.20196, KL=0.06957 | Val: loss=0.28049, recon=0.27810, KL=0.00796\n",
      "Epoch 055 | Train: loss=0.22252, recon=0.20035, KL=0.07389 | Val: loss=0.27832, recon=0.27618, KL=0.00713\n",
      "Epoch 056 | Train: loss=0.22081, recon=0.19857, KL=0.07413 | Val: loss=0.26921, recon=0.26705, KL=0.00718\n",
      "Epoch 057 | Train: loss=0.22282, recon=0.20428, KL=0.06182 | Val: loss=0.27313, recon=0.27167, KL=0.00484\n",
      "Epoch 058 | Train: loss=0.21306, recon=0.19131, KL=0.07251 | Val: loss=0.26746, recon=0.26572, KL=0.00582\n",
      "Epoch 059 | Train: loss=0.21295, recon=0.19530, KL=0.05883 | Val: loss=0.26747, recon=0.26562, KL=0.00618\n",
      "Epoch 060 | Train: loss=0.21669, recon=0.19680, KL=0.06630 | Val: loss=0.26971, recon=0.26800, KL=0.00569\n",
      "Epoch 061 | Train: loss=0.21621, recon=0.19944, KL=0.05593 | Val: loss=0.27249, recon=0.27123, KL=0.00418\n",
      "Epoch 062 | Train: loss=0.21180, recon=0.19662, KL=0.05062 | Val: loss=0.26822, recon=0.26690, KL=0.00440\n",
      "Epoch 063 | Train: loss=0.21122, recon=0.19508, KL=0.05381 | Val: loss=0.27029, recon=0.26904, KL=0.00418\n",
      "Epoch 064 | Train: loss=0.21405, recon=0.19769, KL=0.05452 | Val: loss=0.27538, recon=0.27429, KL=0.00364\n",
      "Epoch 065 | Train: loss=0.20671, recon=0.19018, KL=0.05510 | Val: loss=0.26286, recon=0.26137, KL=0.00499\n",
      "Epoch 066 | Train: loss=0.20931, recon=0.19448, KL=0.04942 | Val: loss=0.27162, recon=0.27043, KL=0.00394\n",
      "Epoch 067 | Train: loss=0.21486, recon=0.20007, KL=0.04930 | Val: loss=0.26153, recon=0.26044, KL=0.00364\n",
      "Epoch 068 | Train: loss=0.21935, recon=0.20397, KL=0.05129 | Val: loss=0.27281, recon=0.27158, KL=0.00409\n",
      "Epoch 069 | Train: loss=0.25472, recon=0.23901, KL=0.05239 | Val: loss=0.28817, recon=0.28675, KL=0.00473\n",
      "Epoch 070 | Train: loss=0.21204, recon=0.19687, KL=0.05057 | Val: loss=0.27104, recon=0.26931, KL=0.00577\n",
      "Epoch 071 | Train: loss=0.21249, recon=0.19819, KL=0.04767 | Val: loss=0.26554, recon=0.26394, KL=0.00533\n",
      "Epoch 072 | Train: loss=0.20636, recon=0.19263, KL=0.04577 | Val: loss=0.27135, recon=0.26977, KL=0.00526\n",
      "Epoch 073 | Train: loss=0.21172, recon=0.19693, KL=0.04930 | Val: loss=0.26681, recon=0.26513, KL=0.00559\n",
      "Epoch 074 | Train: loss=0.20078, recon=0.18773, KL=0.04348 | Val: loss=0.27016, recon=0.26869, KL=0.00491\n",
      "Epoch 075 | Train: loss=0.20848, recon=0.19470, KL=0.04594 | Val: loss=0.27172, recon=0.27011, KL=0.00536\n",
      "Epoch 076 | Train: loss=0.22078, recon=0.20812, KL=0.04220 | Val: loss=0.26187, recon=0.26057, KL=0.00431\n",
      "Epoch 077 | Train: loss=0.20864, recon=0.19646, KL=0.04062 | Val: loss=0.25750, recon=0.25627, KL=0.00411\n",
      "Epoch 078 | Train: loss=0.20830, recon=0.19635, KL=0.03984 | Val: loss=0.26547, recon=0.26430, KL=0.00388\n",
      "Epoch 079 | Train: loss=0.24078, recon=0.22889, KL=0.03965 | Val: loss=0.27036, recon=0.26919, KL=0.00392\n",
      "Epoch 080 | Train: loss=0.19548, recon=0.18535, KL=0.03377 | Val: loss=0.26725, recon=0.26640, KL=0.00283\n",
      "Epoch 081 | Train: loss=0.20261, recon=0.19045, KL=0.04054 | Val: loss=0.27030, recon=0.26938, KL=0.00308\n",
      "Epoch 082 | Train: loss=0.20365, recon=0.19371, KL=0.03315 | Val: loss=0.26866, recon=0.26767, KL=0.00332\n",
      "Epoch 083 | Train: loss=0.20219, recon=0.19105, KL=0.03713 | Val: loss=0.26831, recon=0.26757, KL=0.00246\n",
      "Epoch 084 | Train: loss=0.21131, recon=0.20087, KL=0.03480 | Val: loss=0.26880, recon=0.26789, KL=0.00305\n",
      "Epoch 085 | Train: loss=0.21010, recon=0.19971, KL=0.03465 | Val: loss=0.26507, recon=0.26396, KL=0.00372\n",
      "Epoch 086 | Train: loss=0.20178, recon=0.19221, KL=0.03191 | Val: loss=0.26881, recon=0.26760, KL=0.00406\n",
      "Epoch 087 | Train: loss=0.20502, recon=0.19490, KL=0.03370 | Val: loss=0.26446, recon=0.26317, KL=0.00429\n",
      "Epoch 088 | Train: loss=0.20679, recon=0.19601, KL=0.03594 | Val: loss=0.27395, recon=0.27274, KL=0.00405\n",
      "Epoch 089 | Train: loss=0.19608, recon=0.18727, KL=0.02938 | Val: loss=0.26287, recon=0.26189, KL=0.00327\n",
      "Epoch 090 | Train: loss=0.19941, recon=0.18966, KL=0.03250 | Val: loss=0.26495, recon=0.26406, KL=0.00298\n",
      "Epoch 091 | Train: loss=0.19131, recon=0.18265, KL=0.02887 | Val: loss=0.26032, recon=0.25959, KL=0.00246\n",
      "Epoch 092 | Train: loss=0.20285, recon=0.19413, KL=0.02904 | Val: loss=0.26577, recon=0.26504, KL=0.00244\n",
      "Epoch 093 | Train: loss=0.19593, recon=0.18702, KL=0.02971 | Val: loss=0.26760, recon=0.26660, KL=0.00331\n",
      "Epoch 094 | Train: loss=0.19333, recon=0.18563, KL=0.02568 | Val: loss=0.26108, recon=0.25990, KL=0.00394\n",
      "Epoch 095 | Train: loss=0.19548, recon=0.18653, KL=0.02983 | Val: loss=0.26533, recon=0.26421, KL=0.00373\n",
      "Epoch 096 | Train: loss=0.19316, recon=0.18471, KL=0.02816 | Val: loss=0.26870, recon=0.26765, KL=0.00349\n",
      "Epoch 097 | Train: loss=0.19243, recon=0.18512, KL=0.02438 | Val: loss=0.25883, recon=0.25806, KL=0.00255\n",
      "Epoch 098 | Train: loss=0.18880, recon=0.18015, KL=0.02884 | Val: loss=0.25975, recon=0.25891, KL=0.00282\n",
      "Epoch 099 | Train: loss=0.20163, recon=0.19417, KL=0.02488 | Val: loss=0.26459, recon=0.26397, KL=0.00207\n",
      "Epoch 100 | Train: loss=0.20511, recon=0.19725, KL=0.02618 | Val: loss=0.26742, recon=0.26671, KL=0.00234\n",
      "Epoch 101 | Train: loss=0.19008, recon=0.18256, KL=0.02504 | Val: loss=0.26995, recon=0.26940, KL=0.00183\n",
      "Epoch 102 | Train: loss=0.19728, recon=0.19018, KL=0.02367 | Val: loss=0.26306, recon=0.26239, KL=0.00226\n",
      "Epoch 103 | Train: loss=0.22974, recon=0.22287, KL=0.02292 | Val: loss=0.27503, recon=0.27438, KL=0.00217\n",
      "Epoch 104 | Train: loss=0.23725, recon=0.23098, KL=0.02092 | Val: loss=0.26549, recon=0.26492, KL=0.00189\n",
      "Epoch 105 | Train: loss=0.19340, recon=0.18562, KL=0.02593 | Val: loss=0.27223, recon=0.27158, KL=0.00217\n",
      "Epoch 106 | Train: loss=0.20502, recon=0.19759, KL=0.02476 | Val: loss=0.28431, recon=0.28329, KL=0.00338\n",
      "Epoch 107 | Train: loss=0.19768, recon=0.19059, KL=0.02362 | Val: loss=0.26181, recon=0.26108, KL=0.00244\n",
      "\n",
      "Early stopping at epoch 107. Best epoch was 77 with val_loss=0.25750\n",
      "\n",
      "✅ Loaded best model from: C:\\Users\\ahasa\\project_root\\models\\vae_dense_eclss.pth\n",
      "\n",
      "Computing reconstruction errors on test set...\n",
      "\n",
      "Reconstruction error threshold (99th percentile of train): 1.32124\n",
      "\n",
      "============================================================\n",
      " ANOMALY DETECTION PERFORMANCE (USING RECONSTRUCTION ERROR)\n",
      "============================================================\n",
      "AUC (recon error vs. binary label): 0.8491\n",
      "\n",
      "Threshold-based confusion matrix (99th percentile of train):\n",
      "[[ 18   0]\n",
      " [115  65]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Nominal       0.14      1.00      0.24        18\n",
      "     Anomaly       1.00      0.36      0.53       180\n",
      "\n",
      "    accuracy                           0.42       198\n",
      "   macro avg       0.57      0.68      0.38       198\n",
      "weighted avg       0.92      0.42      0.50       198\n",
      "\n",
      "================ TRAIN NOMINAL DETECTION RESULTS ================\n",
      "Train accuracy (correctly identified nominal): 98.81%\n",
      "Train false-alarm rate: 1.19%\n",
      "=================================================================\n",
      "\n",
      "\n",
      "ROC curve: first 5 points (FPR, TPR, thresh):\n",
      "  0: FPR=0.000, TPR=0.000, thr=inf\n",
      "  1: FPR=0.000, TPR=0.006, thr=101.74361\n",
      "  2: FPR=0.000, TPR=0.594, thr=0.62273\n",
      "  3: FPR=0.056, TPR=0.594, thr=0.62179\n",
      "  4: FPR=0.056, TPR=0.611, thr=0.60889\n",
      "\n",
      "✅ Saved test reconstruction errors to: C:\\Users\\ahasa\\project_root\\data\\eclss_preprocessed\\vae_test_recon_errors.npy\n",
      "✅ Saved test latent μ vectors to: C:\\Users\\ahasa\\project_root\\data\\eclss_preprocessed\\vae_test_latent_mu.npy\n",
      "\n",
      "Done. You can now:\n",
      "  • Use 'vae_test_recon_errors.npy' to tune thresholds / plot ROC.\n",
      "  • Use 'vae_test_latent_mu.npy' as features for SVM fault classifier.\n",
      "  • Optionally visualize latent space with t-SNE or PCA.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_vae_eclss.py\n",
    "\n",
    "Dense (MLP) Variational Autoencoder for the ECLSS synthetic dataset.\n",
    "\n",
    "Pipeline:\n",
    "  1) Load preprocessed data from data/eclss_preprocessed/\n",
    "        - X_train_nom_flat.npy      (nominal training, flattened, scaled)\n",
    "        - X_val_nom_flat.npy        (nominal validation, flattened, scaled)\n",
    "        - X_test_all_flat.npy       (nominal + faulty, flattened, scaled)\n",
    "        - y_test_all_binary.npy     (0 = nominal, 1 = anomaly)\n",
    "  2) Define a fully-connected VAE:\n",
    "        - Encoder: 3000 → 512 → 256 → 128 → (μ, logσ²) in ℝ^latent_dim\n",
    "        - Decoder: latent_dim → 128 → 256 → 512 → 3000\n",
    "  3) Train on NOMINAL data only (train set).\n",
    "  4) Use validation loss for early stopping.\n",
    "  5) Evaluate anomaly detection on test set using reconstruction error.\n",
    "  6) Save:\n",
    "        - Best VAE weights: models/vae_dense.pth\n",
    "        - Per-sample reconstruction errors for test set: .npy\n",
    "        - Latent vectors for test set (for later SVM): .npy\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# PATH HANDLING\n",
    "# ============================================================\n",
    "\n",
    "# If running as script, __file__ is defined; inside a notebook, fall back to cwd\n",
    "try:\n",
    "    REPO_ROOT = Path(__file__).resolve().parents[1]\n",
    "except NameError:\n",
    "    REPO_ROOT = Path.cwd()\n",
    "\n",
    "DATA_ROOT = REPO_ROOT / \"data\"\n",
    "PRE_DIR = DATA_ROOT / \"eclss_preprocessed\"\n",
    "MODEL_DIR = REPO_ROOT / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class VAEConfig:\n",
    "    # Data dimensions\n",
    "    input_dim: int = 3000          # 1000 timesteps × 3 sensors, flattened\n",
    "    latent_dim: int = 32           # larger latent space\n",
    "\n",
    "    # Hidden layer sizes (encoder; decoder mirrors this list)\n",
    "    hidden_dims: Tuple[int, ...] = (1024, 512, 256)\n",
    "\n",
    "    # Training hyperparameters\n",
    "    batch_size: int = 8\n",
    "    num_epochs: int = 200\n",
    "    learning_rate: float = 1e-3\n",
    "    beta: float = 0.3              # \n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping_patience: int = 30\n",
    "\n",
    "    # Device\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "\n",
    "cfg = VAEConfig()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DATASET WRAPPER\n",
    "# ============================================================\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple Dataset wrapper around a 2D numpy array (N, D).\n",
    "\n",
    "    We already pre-flattened and normalized the data, so each row is\n",
    "    a 3000-dimensional feature vector (1000 timesteps × 3 sensors).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X: np.ndarray):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        return self.X[idx]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DENSE VAE DEFINITION\n",
    "# ============================================================\n",
    "\n",
    "class DenseVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully-connected Variational Autoencoder.\n",
    "\n",
    "    Encoder:\n",
    "        x ∈ ℝ^D\n",
    "          → hidden layers (Linear + BatchNorm + LeakyReLU + Dropout)\n",
    "          → h ∈ ℝ^{hidden_dims[-1]}\n",
    "          → μ, logσ² ∈ ℝ^{latent_dim}\n",
    "\n",
    "    Decoder:\n",
    "        z ∈ ℝ^{latent_dim}\n",
    "          → hidden layers (mirror of encoder)\n",
    "          → x̂ ∈ ℝ^D  (reconstructed, same dimensionality as input)\n",
    "\n",
    "    We train with:\n",
    "        L = reconstruction_loss + β * KL_divergence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dims: Tuple[int, ...], latent_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # -------------------------\n",
    "        # Encoder network\n",
    "        # -------------------------\n",
    "        encoder_layers = []\n",
    "        in_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            encoder_layers.append(nn.Linear(in_dim, h_dim))\n",
    "            encoder_layers.append(nn.BatchNorm1d(h_dim))\n",
    "            encoder_layers.append(nn.LeakyReLU(0.2))\n",
    "            encoder_layers.append(nn.Dropout(0.1))\n",
    "            in_dim = h_dim\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Final linear layers to output μ and logσ²\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "\n",
    "        # -------------------------\n",
    "        # Decoder network\n",
    "        # -------------------------\n",
    "        decoder_layers = []\n",
    "        # Start by mapping latent z up to the last hidden size\n",
    "        decoder_layers.append(nn.Linear(latent_dim, hidden_dims[-1]))\n",
    "        decoder_layers.append(nn.BatchNorm1d(hidden_dims[-1]))\n",
    "        decoder_layers.append(nn.LeakyReLU(0.2))\n",
    "\n",
    "        # Then mirror the hidden_dims backwards\n",
    "        reversed_hidden = list(hidden_dims[::-1])\n",
    "        in_dim = reversed_hidden[0]\n",
    "        for h_dim in reversed_hidden[1:]:\n",
    "            decoder_layers.append(nn.Linear(in_dim, h_dim))\n",
    "            decoder_layers.append(nn.BatchNorm1d(h_dim))\n",
    "            decoder_layers.append(nn.LeakyReLU(0.2))\n",
    "            in_dim = h_dim\n",
    "\n",
    "        # Final layer back to input_dim, no activation\n",
    "        decoder_layers.append(nn.Linear(in_dim, input_dim))\n",
    "\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    # ---------- VAE core methods ----------\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Map input x → encoder hidden representation → (μ, logσ²).\n",
    "        \"\"\"\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick:\n",
    "            z = μ + σ ⊙ ε,    ε ~ N(0, I)\n",
    "\n",
    "        Allows gradients to flow through μ and logσ².\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Map latent code z back to reconstruction x̂.\n",
    "        \"\"\"\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Full VAE forward pass:\n",
    "            x → (μ, logσ²) → z → x̂\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mu, logvar\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LOSS FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def vae_loss(\n",
    "    x: torch.Tensor,\n",
    "    x_hat: torch.Tensor,\n",
    "    mu: torch.Tensor,\n",
    "    logvar: torch.Tensor,\n",
    "    beta: float = 1.0,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute VAE loss: reconstruction + β * KL.\n",
    "\n",
    "    - Reconstruction: mean-squared error (MSE) between x and x̂.\n",
    "      Inputs are standardized (zero-mean, unit-variance), so MSE is natural.\n",
    "\n",
    "    - KL divergence between q(z|x) = N(μ,σ²) and p(z) = N(0,I):\n",
    "        KL = -0.5 * Σ (1 + logσ² - μ² - σ²)\n",
    "\n",
    "    Returns:\n",
    "        total_loss, recon_loss, kl_loss\n",
    "    \"\"\"\n",
    "    # Mean squared error over features, averaged over batch\n",
    "    recon_loss = F.mse_loss(x_hat, x, reduction=\"mean\")\n",
    "\n",
    "    # KL divergence (average over batch)\n",
    "    kl_element = 1 + logvar - mu.pow(2) - logvar.exp()\n",
    "    kl_loss = -0.5 * torch.mean(torch.sum(kl_element, dim=1))\n",
    "\n",
    "    total_loss = recon_loss + beta * kl_loss\n",
    "    return total_loss, recon_loss, kl_loss\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING / EVAL LOOPS\n",
    "# ============================================================\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: DenseVAE,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: str,\n",
    "    beta: float,\n",
    ") -> Tuple[float, float, float]:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_recon = 0.0\n",
    "    total_kl = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for x in loader:\n",
    "        x = x.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_hat, mu, logvar = model(x)\n",
    "        loss, recon, kl = vae_loss(x, x_hat, mu, logvar, beta=beta)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_recon += recon.item()\n",
    "        total_kl += kl.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    return (\n",
    "        total_loss / n_batches,\n",
    "        total_recon / n_batches,\n",
    "        total_kl / n_batches,\n",
    "    )\n",
    "\n",
    "\n",
    "def eval_one_epoch(\n",
    "    model: DenseVAE,\n",
    "    loader: DataLoader,\n",
    "    device: str,\n",
    "    beta: float,\n",
    ") -> Tuple[float, float, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_recon = 0.0\n",
    "    total_kl = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x in loader:\n",
    "            x = x.to(device)\n",
    "            x_hat, mu, logvar = model(x)\n",
    "            loss, recon, kl = vae_loss(x, x_hat, mu, logvar, beta=beta)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_recon += recon.item()\n",
    "            total_kl += kl.item()\n",
    "            n_batches += 1\n",
    "\n",
    "    return (\n",
    "        total_loss / n_batches,\n",
    "        total_recon / n_batches,\n",
    "        total_kl / n_batches,\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ANOMALY SCORING\n",
    "# ============================================================\n",
    "\n",
    "def compute_reconstruction_errors(\n",
    "    model: DenseVAE,\n",
    "    X: np.ndarray,\n",
    "    device: str,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute per-sample reconstruction error and latent vectors.\n",
    "\n",
    "    Args:\n",
    "        model: trained VAE\n",
    "        X:     numpy array (N, D)\n",
    "    Returns:\n",
    "        errors: np.array of shape (N,), MSE per sample\n",
    "        Z:      np.array of shape (N, latent_dim), latent mean μ\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    dataset = NumpyDataset(X)\n",
    "    loader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "    all_errors = []\n",
    "    all_mu = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x in loader:\n",
    "            x = x.to(device)\n",
    "            x_hat, mu, logvar = model(x)\n",
    "\n",
    "            # MSE per sample (mean over features)\n",
    "            mse = F.mse_loss(x_hat, x, reduction=\"none\")\n",
    "            mse_per_sample = mse.mean(dim=1).cpu().numpy()\n",
    "\n",
    "            all_errors.append(mse_per_sample)\n",
    "            all_mu.append(mu.cpu().numpy())\n",
    "\n",
    "    errors = np.concatenate(all_errors, axis=0)\n",
    "    Z = np.concatenate(all_mu, axis=0)\n",
    "    return errors, Z\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    print(\"============================================================\")\n",
    "    print(\" DENSE VAE TRAINING FOR ECLSS ANOMALY DETECTION\")\n",
    "    print(\"============================================================\")\n",
    "    print(f\"Using device: {cfg.device}\")\n",
    "    print(f\"Preprocessed data dir: {PRE_DIR}\\n\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1) LOAD PREPROCESSED DATA\n",
    "    # --------------------------------------------------------\n",
    "    X_train_nom = np.load(PRE_DIR / \"X_train_nom_flat.npy\")\n",
    "    X_val_nom = np.load(PRE_DIR / \"X_val_nom_flat.npy\")\n",
    "    X_test_all = np.load(PRE_DIR / \"X_test_all_flat.npy\")\n",
    "    y_test_all_binary = np.load(PRE_DIR / \"y_test_all_binary.npy\")  # 0=nominal, 1=anomaly\n",
    "\n",
    "    print(\"Shapes:\")\n",
    "    print(f\"  X_train_nom: {X_train_nom.shape}\")\n",
    "    print(f\"  X_val_nom:   {X_val_nom.shape}\")\n",
    "    print(f\"  X_test_all:  {X_test_all.shape}\")\n",
    "    print(f\"  y_test_all:  {y_test_all_binary.shape}\\n\")\n",
    "\n",
    "    # Wrap in Datasets / DataLoaders\n",
    "    train_dataset = NumpyDataset(X_train_nom)\n",
    "    val_dataset = NumpyDataset(X_val_nom)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=cfg.batch_size, shuffle=True, drop_last=False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=cfg.batch_size, shuffle=False, drop_last=False\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2) INITIALIZE VAE\n",
    "    # --------------------------------------------------------\n",
    "    model = DenseVAE(\n",
    "        input_dim=cfg.input_dim,\n",
    "        hidden_dims=cfg.hidden_dims,\n",
    "        latent_dim=cfg.latent_dim,\n",
    "    ).to(cfg.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)\n",
    "\n",
    "    print(model)\n",
    "    print(\"\\nNumber of trainable parameters:\",\n",
    "          sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    print()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3) TRAIN WITH EARLY STOPPING (ON VALIDATION LOSS)\n",
    "    # --------------------------------------------------------\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    patience_counter = 0\n",
    "    best_model_path = MODEL_DIR / \"vae_dense_eclss.pth\"\n",
    "\n",
    "    for epoch in range(1, cfg.num_epochs + 1):\n",
    "        train_loss, train_recon, train_kl = train_one_epoch(\n",
    "            model, train_loader, optimizer, cfg.device, cfg.beta\n",
    "        )\n",
    "        val_loss, val_recon, val_kl = eval_one_epoch(\n",
    "            model, val_loader, cfg.device, cfg.beta\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"Train: loss={train_loss:.5f}, recon={train_recon:.5f}, KL={train_kl:.5f} | \"\n",
    "            f\"Val: loss={val_loss:.5f}, recon={val_recon:.5f}, KL={val_kl:.5f}\"\n",
    "        )\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss - 1e-4:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= cfg.early_stopping_patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch}. \"\n",
    "                      f\"Best epoch was {best_epoch} with val_loss={best_val_loss:.5f}\")\n",
    "                break\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=cfg.device))\n",
    "    print(f\"\\n✅ Loaded best model from: {best_model_path}\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 4) ANOMALY DETECTION EVALUATION ON TEST SET\n",
    "    # --------------------------------------------------------\n",
    "    print(\"\\nComputing reconstruction errors on test set...\")\n",
    "    test_errors, test_latent = compute_reconstruction_errors(\n",
    "        model, X_test_all, cfg.device\n",
    "    )\n",
    "\n",
    "    # Simple threshold: use 99th percentile of TRAIN reconstruction errors\n",
    "    train_errors, _ = compute_reconstruction_errors(model, X_train_nom, cfg.device)\n",
    "    threshold = np.quantile(train_errors, 0.99)\n",
    "\n",
    "    print(f\"\\nReconstruction error threshold (99th percentile of train): {threshold:.5f}\")\n",
    "\n",
    "    y_pred_binary = (test_errors > threshold).astype(int)\n",
    "\n",
    "    # Metrics\n",
    "    auc = roc_auc_score(y_test_all_binary, test_errors)\n",
    "    cm = confusion_matrix(y_test_all_binary, y_pred_binary)\n",
    "    report = classification_report(\n",
    "        y_test_all_binary, y_pred_binary, target_names=[\"Nominal\", \"Anomaly\"]\n",
    "    )\n",
    "\n",
    "    print(\"\\n============================================================\")\n",
    "    print(\" ANOMALY DETECTION PERFORMANCE (USING RECONSTRUCTION ERROR)\")\n",
    "    print(\"============================================================\")\n",
    "    print(f\"AUC (recon error vs. binary label): {auc:.4f}\")\n",
    "    print(\"\\nThreshold-based confusion matrix (99th percentile of train):\")\n",
    "    print(cm)\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(report)\n",
    "    # ============================================================\n",
    "# TRAIN NOMINAL “ACCURACY” (false-alarm analysis)\n",
    "# ============================================================\n",
    "\n",
    "    train_pred_nominal = (train_errors <= threshold).astype(int)  # 1 = nominal\n",
    "    train_accuracy = train_pred_nominal.mean() * 100\n",
    "    false_alarm_rate = 100 - train_accuracy\n",
    "\n",
    "    print(\"================ TRAIN NOMINAL DETECTION RESULTS ================\")\n",
    "    print(f\"Train accuracy (correctly identified nominal): {train_accuracy:.2f}%\")\n",
    "    print(f\"Train false-alarm rate: {false_alarm_rate:.2f}%\")\n",
    "    print(\"=================================================================\\n\")\n",
    "    # Also show approximate ROC operating point for reference\n",
    "    fpr, tpr, roc_thresh = roc_curve(y_test_all_binary, test_errors)\n",
    "    print(f\"\\nROC curve: first 5 points (FPR, TPR, thresh):\")\n",
    "    for i in range(min(5, len(fpr))):\n",
    "        print(f\"  {i}: FPR={fpr[i]:.3f}, TPR={tpr[i]:.3f}, thr={roc_thresh[i]:.5f}\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 5) SAVE ERRORS & LATENT VECTORS (FOR SVM, PLOTS, ETC.)\n",
    "    # --------------------------------------------------------\n",
    "    np.save(PRE_DIR / \"vae_test_recon_errors.npy\", test_errors)\n",
    "    np.save(PRE_DIR / \"vae_test_latent_mu.npy\", test_latent)\n",
    "\n",
    "    print(f\"\\n✅ Saved test reconstruction errors to: \"\n",
    "          f\"{(PRE_DIR / 'vae_test_recon_errors.npy').resolve()}\")\n",
    "    print(f\"✅ Saved test latent μ vectors to: \"\n",
    "          f\"{(PRE_DIR / 'vae_test_latent_mu.npy').resolve()}\")\n",
    "\n",
    "    print(\"\\nDone. You can now:\")\n",
    "    print(\"  • Use 'vae_test_recon_errors.npy' to tune thresholds / plot ROC.\")\n",
    "    print(\"  • Use 'vae_test_latent_mu.npy' as features for SVM fault classifier.\")\n",
    "    print(\"  • Optionally visualize latent space with t-SNE or PCA.\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce18dc25-35e3-447b-9080-d92eee813a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
